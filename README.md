# ğŸ–ï¸ AI-Based Invisible Hand for Disabled People

**An intelligent computer control system for people with disabilities using facial gesturesâ€”blink to click, head tilt to move cursor, and open mouth to scroll. Powered by Computer Vision and Python. No hardware, just a webcam!**

---

## ğŸ” Overview

This project presents a hands-free, AI-powered interface that allows usersâ€”especially individuals with motor disabilitiesâ€”to interact with a computer using facial gestures. It uses real-time facial landmark detection to control mouse movements and simulate click and scroll actions.

### ğŸ¯ Key Features
- ğŸ‘ï¸ Blink left/right eye to simulate mouse left/right click  
- ğŸ§  Head tilt controls cursor movement direction  
- ğŸ‘„ Open mouth to trigger scrolling mode (up/down based on nose position)  
- ğŸ“ˆ Calibration-based EAR & MAR threshold tuning for gesture accuracy  
- âš¡ Real-time processing using only a webcam

---

## ğŸ› ï¸ Technologies Used

- Python 3
- OpenCV
- Dlib (with shape predictor)
- PyAutoGUI
- Imutils
- NumPy, SciPy, Matplotlib

---

## âš™ï¸ Installation

1. **Install dependencies**

   pip install -r requirements.txt
Download shape predictor 

(Due to size constraints, this file isn't stored in the repo. You can download it from: http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2)

Place shape_predictor_68_face_landmarks.dat in the root directory

Run the application

python project.py
ğŸ“ Folder Structure
invisible-hand/
â”‚
â”œâ”€â”€ project.py               # Main calibration + real-time control logic
â”œâ”€â”€ requirements.txt         # Python package requirements
â”œâ”€â”€ shape_predictor.dat      # Dlib face landmark model (external)
â”œâ”€â”€ VDP.py                   # Color-based drawing (experimental)
â”œâ”€â”€ hough.py                 # Hough line transform demo (experimental)
ğŸ§ª How It Works
Phase I â€“ Calibration (First 25 seconds)
Records EAR and MAR values for:

Both eyes open

Left eye blink

Right eye blink

Open mouth

Displays real-time plots to visualize gesture thresholds

Phase II â€“ Gesture Control
Uses calibrated EAR & MAR values to trigger:

Left click (left eye blink)

Right click (right eye blink)

Mouse movement (nose tracking)

Scroll mode (mouth open toggle + head tilt)

ğŸ§  Algorithms Used
EAR (Eye Aspect Ratio) â€“ to detect eye blinks

MAR (Mouth Aspect Ratio) â€“ to detect mouth opening

68-point facial landmark detection â€“ via Dlib

Angle calculation â€“ nose to screen center vector for direction

PyAutoGUI â€“ to simulate actual mouse and scroll events

âœ… Applications
Assistive technology for people with motor disabilities

Hands-free computer control in public/sterile environments

Accessibility tools for UI testing and remote systems

Gamified accessibility for education and entertainment

