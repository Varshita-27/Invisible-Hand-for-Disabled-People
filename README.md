
# ğŸ–ï¸ AI-Based Invisible Hand for Disabled People

**An intelligent computer control system for people with disabilities using facial gesturesâ€”blink to click, head tilt to move cursor, and open mouth to scroll. Powered by Computer Vision and Python. No hardware, just a webcam!**

---

## ğŸ” Overview

This project presents a hands-free, AI-powered interface that allows usersâ€”especially individuals with motor disabilitiesâ€”to interact with a computer using facial gestures. It uses real-time facial landmark detection to control mouse movements and simulate click and scroll actions.

### ğŸ¯ Key Features

- ğŸ‘ï¸ Blink left/right eye to simulate mouse left/right click  
- ğŸ§  Head tilt controls cursor movement direction  
- ğŸ‘„ Open mouth to trigger scrolling mode (up/down based on nose position)  
- ğŸ“ˆ Calibration-based EAR & MAR threshold tuning for gesture accuracy  
- âš¡ Real-time processing using only a webcam

---

## ğŸ› ï¸ Technologies Used

- Python 3
- OpenCV
- Dlib (with shape predictor)
- PyAutoGUI
- Imutils
- NumPy, SciPy, Matplotlib

---

## âš™ï¸ Installation & Setup

1. **Install dependencies**

   Use the provided `requirements.txt` file to install all necessary Python packages:

   ```bash
   pip install -r requirements.txt
````

2. **Download and place the shape predictor file**

   Download the `shape_predictor_68_face_landmarks.dat` model from the [official dlib website](http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2), extract it, and place it in the root folder of this project (same level as `project.py`).

3. **Run the application**

   Simply run the main Python file:

   ```bash
   python project.py
   ```

---

## ğŸ“ Project Structure

```
invisible-hand/
â”‚
â”œâ”€â”€ project.py               # Main calibration + real-time gesture control code
â”œâ”€â”€ requirements.txt         # All required Python libraries
â”œâ”€â”€ shape_predictor.dat      # Dlibâ€™s face landmark model (external file)
â”œâ”€â”€ VDP.py                   # Color-based drawing (optional, experimental)
â”œâ”€â”€ hough.py                 # Hough line transform demo (optional, experimental)
```

---

## ğŸ§ª How It Works

### ğŸ”¹ Phase I â€“ Calibration (First 25 seconds)

* Records facial gesture data for:

  * Both eyes open
  * Left eye closed (for left click)
  * Right eye closed (for right click)
  * Mouth open (for scrolling)
* Plots real-time graphs to visualize EAR and MAR values
* Calculates median thresholds for each gesture

### ğŸ”¹ Phase II â€“ Real-Time Gesture Control

* Tracks facial landmarks using webcam
* Calculates EAR (Eye Aspect Ratio) & MAR (Mouth Aspect Ratio)
* Uses the calibrated thresholds to trigger:

  * ğŸ–±ï¸ Left click â†’ left eye blink
  * ğŸ–±ï¸ Right click â†’ right eye blink
  * â• Mouse movement â†’ nose direction from screen center
  * ğŸ”ƒ Scroll mode toggle â†’ open mouth
  * â¬‡ï¸â¬†ï¸ Scroll direction â†’ nose up/down tilt while in scroll mode

---

## ğŸ§  Algorithms & Logic

* **EAR (Eye Aspect Ratio)**: Detects eye closure or blink
* **MAR (Mouth Aspect Ratio)**: Detects mouth open gesture
* **Facial Landmarks**: Extracted using Dlibâ€™s 68-point face shape predictor
* **Convex Hulls**: Used to outline facial features like eyes and mouth
* **Angle Detection**: Nose tip relative to center to detect direction of movement
* **PyAutoGUI**: Simulates real mouse movements and clicks

---

## âœ… Applications

* ğŸ’» Assistive technology for people with motor or physical disabilities
* ğŸ§ª Touchless computer interface for labs, hospitals, or cleanrooms
* ğŸ•¹ï¸ Accessible gaming for people with limited mobility
* ğŸŒ Hands-free navigation for public kiosks or accessibility UI testing

---
